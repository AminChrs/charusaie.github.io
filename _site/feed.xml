<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/blog/" rel="alternate" type="text/html" /><updated>2020-08-28T21:21:51+02:00</updated><id>http://localhost:4000/blog/feed.xml</id><title type="html">Math Learner</title><subtitle>Probably and approximately two-weekly expression of my ideas on math and ML.</subtitle><entry><title type="html">Weme to Jekyll!</title><link href="http://localhost:4000/blog/jekyll/update/2020/08/28/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Weme to Jekyll!" /><published>2020-08-28T18:24:00+02:00</published><updated>2020-08-28T18:24:00+02:00</updated><id>http://localhost:4000/blog/jekyll/update/2020/08/28/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/blog/jekyll/update/2020/08/28/welcome-to-jekyll.html">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;Jekyll requires blog post files to be named according to the following format:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YEAR-MONTH-DAY-title.MARKUP&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YEAR&lt;/code&gt; is a four-digit number, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MONTH&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DAY&lt;/code&gt; are both two-digit numbers, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MARKUP&lt;/code&gt; is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry><entry><title type="html">My new post :)</title><link href="http://localhost:4000/blog/catagory%201/catagory%202/2020/08/28/A-new-post-on-my-blog.html" rel="alternate" type="text/html" title="My new post :)" /><published>2020-08-28T00:00:00+02:00</published><updated>2020-08-28T00:00:00+02:00</updated><id>http://localhost:4000/blog/catagory%201/catagory%202/2020/08/28/A-new-post-on-my-blog</id><content type="html" xml:base="http://localhost:4000/blog/catagory%201/catagory%202/2020/08/28/A-new-post-on-my-blog.html">&lt;p&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/p&gt;
&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; lang=&quot;&quot; xml:lang=&quot;&quot;&gt;
&lt;head&gt;
  &lt;meta charset=&quot;utf-8&quot; /&gt;
  &lt;meta name=&quot;generator&quot; content=&quot;pandoc&quot; /&gt;
  &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0, user-scalable=yes&quot; /&gt;
  &lt;title&gt;Some notes on DP-MERF&lt;/title&gt;
  &lt;style type=&quot;text/css&quot;&gt;
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  &lt;/style&gt;
  &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
  &lt;!--[if lt IE 9]&gt;
    &lt;script src=&quot;//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js&quot;&gt;&lt;/script&gt;
  &lt;![endif]--&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;header&gt;
&lt;h1 class=&quot;title&quot;&gt;Some notes on DP-MERF&lt;/h1&gt;
&lt;/header&gt;

&lt;h1 id=&quot;add-noise-to-the-high-dimensional-mean-embedding&quot;&gt;Add noise to the high-dimensional mean embedding&lt;/h1&gt;
&lt;p&gt;Adding the Gaussian noise to privatize a vector has been done in the literature, and also is the core basis of privatization in DP-MERF. The idea is to add the noise to the empirical mean embedding once, and then using that privatized mean embedding to find MMD between the real data and generated data, and optimizing the generated data distribution based on that.&lt;/p&gt;
&lt;p&gt;However, the finite-dimensional feature map for a point and based on a characteristic kernel often (if not always) is not achievable. Hence, in DP-MERF, we use a Random Fourier Feature map for shift-invariant kernels. The dimension of those feature maps can be boundlessly large, and as we tend this dimension to the infinity we have a better perception of the data, since we acquisite more frequency amplitude of the data.&lt;/p&gt;

 &lt;!-- more --&gt; 
&lt;p&gt;I show the use of such high-dimensional presentation of each point is not free-of-charge. To prove that, I show the amount of added noise to the mean embedding must be going large proportionately with the dimension of the feature map to privatize the mean embedding. Hence, the mean embedding which is bounded in each element is bearing a noise with variance directly proportional to that bound and the dimension of points feature maps.&lt;/p&gt;
&lt;p&gt;To begin with, here I recall the theorem about Gaussian mechanism.&lt;/p&gt;
&lt;p&gt;Suppose that, for a positive-definite matrix &lt;span class=&quot;math inline&quot;&gt;\(M\in \mathbb{R}^{d\times d}\)&lt;/span&gt;, the family of vectors &lt;span class=&quot;math inline&quot;&gt;\(\{v_D: D\in {\cal D}\}\subset \mathbb{R}^d\)&lt;/span&gt; satisfies &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
        \sup_{D\sim D&amp;#39;}\|M^{-1/2} (v_D-v_{D&amp;#39;})\|_2 \leq \Delta,
         \end{aligned}\]&lt;/span&gt; Then the randomized algorithm which, for the input database &lt;span class=&quot;math inline&quot;&gt;\(D\)&lt;/span&gt; outputs &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
        \tilde{v_D} = v_D + \frac{\Delta}{\sqrt{\epsilon}} Z, \qquad \mathbf{Z}\sim {\cal N}_d (0, M),
     \end{aligned}\]&lt;/span&gt; achieves &lt;span class=&quot;math inline&quot;&gt;\((\alpha, \frac{\alpha\epsilon}{2})\)&lt;/span&gt;-RDP.&lt;/p&gt;
&lt;p&gt;The above theorem has a particular property that the noise does not have anything to do with the dimension of the vector. As an instance, if we have &lt;span class=&quot;math inline&quot;&gt;\(M=I\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(\Delta=1\)&lt;/span&gt; (i.e., &lt;span class=&quot;math inline&quot;&gt;\(\sup_{D\sim D&amp;#39;}\|(v_D-v_{D&amp;#39;})\|\leq 1\)&lt;/span&gt;), the mechanism must add a Gaussian noise to each element with variance &lt;span class=&quot;math inline&quot;&gt;\(\sigma^2 =\frac{1}{\epsilon}\)&lt;/span&gt;. The second moment of the noise is also &lt;span class=&quot;math inline&quot;&gt;\(\mathbb{E}^{1/2}\big[\|\mathbf{Z}/\sqrt{\epsilon}\|^2\big]= \sqrt{\frac{d}{\epsilon}}\)&lt;/span&gt; which could be highly larger than the second-norm sensitivity &lt;span class=&quot;math inline&quot;&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span&gt; Note that this is essentially what happens in DP-MERF and talk about the difference between norm-2 of the whole vector and each pairs and note that the general composition theorem doesn’t distinguish between these two settings. Finally, try to find a better composition theorem in your case or you compare DP-MERF mean embedding privatization with that of covariate shift paper.&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;title&quot;&gt;title&lt;/h2&gt;
&lt;h1 id=&quot;adding-noise-to-mmd&quot;&gt;Adding noise to MMD&lt;/h1&gt;
&lt;p&gt;The DP-MERF algorithm uses MMD as the fundamental element to assimilate the distribution of the output of a generator and the real data. The reason we use MMD is the fact that we can bound the sensitivity of an empirical mean embedding over neighboring datasets. Hence, to privatize the MMD, one can find a finite-dimensional estimation of the empirical mean embedding and add the noise to that vector. This finite-dimensional estimation is obtained by finding the mean of Random Fourier Features of each samples.&lt;/p&gt;
&lt;p&gt;However, this approach is always considering some frequencies in the Fourier transform spectrum of the kernel. This approximation of those feature maps are not characteristic, as we have zeros (almost everywhere) in their Fourier transform.&lt;/p&gt;
&lt;p&gt;To solve this problem, in this note I show that is eligible to add noise to the unbiased/biased estimation of the MMD. To do so, I show the sensitivity of those estimations have efficient bounds.&lt;/p&gt;
&lt;h2 id=&quot;unbiased-estimation&quot;&gt;Unbiased estimation&lt;/h2&gt;
&lt;p&gt;As noted in &lt;span class=&quot;citation&quot; data-cites=&quot;kernel&quot;&gt;&lt;/span&gt;, the unbiased estimation of MMD can be obtained as &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
 {\rm MMD}_u^2 [{\cal F}, X, Y] = &amp;amp;\frac{1}{m(m-1)}\sum_{i=1}^m\sum_{j\neq i} k(x_i, x_j) + \frac{1}{n(n-1)}\sum_{i=1}^n \sum_{j\neq i} k(y_i, y_j)\\&amp;amp;-\frac{2}{mn}\sum_{i=1}^m \sum_{j=1}^n k(x_i, y_j).
     \end{aligned}\]&lt;/span&gt; Hence, if we have a neighboring dataset &lt;span class=&quot;math inline&quot;&gt;\(X&amp;#39;\)&lt;/span&gt; which does not have &lt;span class=&quot;math inline&quot;&gt;\(x_1\)&lt;/span&gt;, and considering the kernel is shift-invariant, then &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
 {\rm MMD}_u^2 [{\cal F}, X, Y]-{\rm MMD}_u^2 [{\cal F}, X&amp;#39;, Y] &amp;amp;= \frac{2}{m(m-1)}\sum_{j=2}^m k(x_1, x_j) - \frac{2}{m n}\sum_{j=1}^n k(x_1, y_j)\\
        &amp;amp;\leq \frac{2}{m} k(0, 0).
     \end{aligned}\]&lt;/span&gt; The reason for the last inequality is that, using the fact that &lt;span class=&quot;math inline&quot;&gt;\(k(\cdot, \cdot)\)&lt;/span&gt; can be represented as an inner product of feature maps of points, and by Cauchy-Schwartz inequality we have &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
        k(x, y)\leq \sqrt{k(x, x) k(y, y)}= k(0, 0),
     \end{aligned}\]&lt;/span&gt; where the equality is based on shift-invariance of &lt;span class=&quot;math inline&quot;&gt;\(k(\cdot, \cdot)\)&lt;/span&gt;. As a result, every average of &lt;span class=&quot;math inline&quot;&gt;\(k(x, y)\)&lt;/span&gt; over &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;s and &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt;s are also less than this value.&lt;/p&gt;
&lt;h2 id=&quot;biased-estimation&quot;&gt;Biased estimation&lt;/h2&gt;
&lt;p&gt;Again, as noted in &lt;span class=&quot;citation&quot; data-cites=&quot;kernel&quot;&gt;&lt;/span&gt;, the biased estimation of MMD is as &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
 {\rm MMD}_b^2 [{\cal F}, X, Y] =  &amp;amp;\frac{1}{m^2}\sum_{i=1}^m\sum_{j=1}^m k(x_i, x_j) + \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n k(y_i, y_j)\\&amp;amp;-\frac{2}{mn}\sum_{i=1}^m \sum_{j=1}^n k(x_i, y_j).
     \end{aligned}\]&lt;/span&gt; Again, using simple calculation, for a neighboring dataset &lt;span class=&quot;math inline&quot;&gt;\(X&amp;#39;\)&lt;/span&gt; which does not have &lt;span class=&quot;math inline&quot;&gt;\(x_1\)&lt;/span&gt;, and for shift-invariant kernels we have &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
 {\rm MMD}_b^2 [{\cal F}, X, Y]-{\rm MMD}_b^2 [{\cal F}, X&amp;#39;, Y] \leq \big(\frac{1}{m^2}+\frac{2}{m}\big) k(0, 0).
     \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;
&lt;p&gt;Using the above discussions, we can simply add noise to the MMD square. However, as we need to have access to different MMD’s (i.e., for different &lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt;s) in optimization steps, we should add noise to the MMD several times which is inefficient.&lt;/p&gt;
&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;
&lt;p&gt;The solution to the above problem is to add dependent noise to preserve the efficiency of the private method. The idea is to consider we have added noise to the infinite-dimensional mean embedding and observe the result of privatized MMD for different &lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt;s. Then see the dependence between the added noise, and finally simulate that noise for different observations.&lt;/p&gt;
&lt;p&gt;Formally, we know that if &lt;span class=&quot;math inline&quot;&gt;\(N(t)\)&lt;/span&gt; is a Gaussian process in the RKHS space, then the biased privatized MMD is as &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
 {\rm MMD}_p^2[{\cal F}, X, Y] &amp;amp;=\big\|\frac{1}{m}\sum k(x_i, \cdot)+N(\cdot)-\frac{1}{n}\sum_{j=1}^n k(y_j, \cdot)\big\|_^2 \\&amp;amp;={\rm MMD}_b^2[{\cal F}, X, Y]+\|N(\cdot)\|_^2\\&amp;amp;~~+2\big\langle N(\cdot), \frac{1}{m}\sum k(x_i, \cdot)-\frac{1}{n}\sum_{j=1}^n k(y_j, \cdot)\big\rangle_.
     \end{aligned}\]&lt;/span&gt; As we need to optimize the distribution of &lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt;, we neglect the second term and have &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
    \widehat{\rm MMD}_p^2[{\cal F}, X, Y] ={\rm MMD}_b^2[{\cal F}, X, Y]+2\big\langle N(\cdot), \frac{1}{m}\sum k(x_i, \cdot)-\frac{1}{n}\sum_{j=1}^n k(y_j, \cdot)\big\rangle_.
     \end{aligned}\]&lt;/span&gt; As a simple fact, if &lt;span class=&quot;math inline&quot;&gt;\(N(\cdot)\)&lt;/span&gt; is a Gaussian process, the added noise to the biased estimation also should be multivariate Gaussian noise. More formally, one can see that &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            \widehat{\rm MMD}_p^2[{\cal F}, X, Y] ={\rm MMD}_b^2[{\cal F}, X, Y]+\frac{2}{m}\sum_{i=1}^m N(x_i)-\frac{2}{n}\sum_{j=1}^n N(y_j).
     \end{aligned}\]&lt;/span&gt; &lt;span&gt; Check the added Gaussian noise to DP-MERF. It seems as the dimension of the approximation grows, the noise will be insanely large for the whole vector. Tend the dimension to infinity to find out that each element of the feature map tends to zero and also the one for the mean embedding. But still the noise you add has a unitary variance for each element.&lt;/span&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name></name></author><category term="Catagory 1" /><category term="Catagory 2" /><category term="Tag 1" /><category term="Tag 2" /><summary type="html">&amp;lt;!DOCTYPE html&amp;gt; Some notes on DP-MERF Some notes on DP-MERF</summary></entry></feed>